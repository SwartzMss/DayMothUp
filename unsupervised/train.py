import torch

'''
K-Means聚类是一种非常流行的无监督学习算法，主要用于将数据集划分成K个集群（或类别）。
这个方法是基于距离的：算法试图最小化每个集群中数据点与其质心（即该集群中所有点的均值）之间的距离。

以下是K-Means算法的基本步骤：
1.初始化：随机选择K个数据点作为初始质心。
2.分配：对于数据集中的每个数据点，根据它与质心的距离，将其分配给最近的质心所在的集群。
3.更新：对于每个集群，计算所有属于该集群的数据点的均值，然后将该均值设置为新的质心。
4.重复：重复分配和更新步骤，直到满足停止条件（例如质心不再显著变化，或者达到预定的最大迭代次数）。

在实际应用中，K-Means算法通常用于市场分割、图像分割、文档聚类等任务。

需要注意的是，K-Means聚类有几个限制：
需要预先设定K值，即集群的数量，这在实际情况中可能并不知道。
它假定所有的集群都是凸的和圆形的，这在实际数据中可能并不成立。
它受初始质心选择的影响较大，不同的初始质心可能导致不同的结果。
'''

# 这是数据集，假设我们有四个二维数据
# 在实际情况下，你可能有更多的数据和更高的维度，例如，图像数据就是高维数据
data = torch.tensor([[1.0, 2.0], [1.0, 3.0], [2.0, 1.0], [3.0, 1.0]])

# 这是类别数，因为我们只有四个数据，所以我们设定类别数为2
k = 2

# 初始化质心，这是K-Means的开始，我们随机选择数据作为初始的质心
centroids = data[torch.randperm(data.size(0))[:k]]
centroids = data[:k]
print(f"centroids = {centroids}")
'''
centroids = tensor([[3., 1.],
        [1., 3.]])
'''

# 这是主要的循环，我们循环10次，但在实际情况中，你可能需要设定一个停止的条件
# 例如，当质心不再显著变化或者达到最大迭代次数
for i in range(1):
    # 这是计算距离，PyTorch允许我们直接减去两个大小不同的tensor，这就是所谓的广播（broadcasting）
    # dists的大小是（数据量，k），即每个数据到每个质心的距离
    dists = torch.norm(data[:, None] - centroids, dim=2)
    print(f"1 = {i}, dists = {dists}")
    # 这是分配步骤，我们计算出每个数据距离哪个质心最近，labels的大小和数据量相同
    labels = dists.argmin(dim=1)
    print("Labels: ", labels)
    # 这是更新步骤，我们根据新分配的类别，更新每个类别的质心
    centroids = torch.stack([data[labels == i].mean(dim=0) for i in range(k)])

# 打印出结果，你应该可以看到质心和每个数据的类别
print("Centroids: ", centroids)
print("Labels: ", labels)




'''
如何理解dists = torch.norm(data[:, None] - centroids, dim=2)
以一个具体的例子来说，假设我们有4个二维数据点：
data = torch.tensor([[1.0, 2.0], [1.0, 3.0], [2.0, 1.0], [3.0, 1.0]])
和两个质心：
centroids = torch.tensor([[3.0, 1.0], [1.0, 3.0]])
然后，我们计算每个数据点到每个质心的欧几里得距离：

dists = torch.norm(data[:, None] - centroids, dim=2)
print(dists)
输出结果将会是一个大小为（4，2）的张量，表示数据集中每个数据点到每个质心的距离：

tensor([[2.2361, 1.0000],
        [2.8284, 0.0000],
        [1.0000, 2.2361],
        [0.0000, 2.8284]])
这个矩阵的每一行表示一个数据点，每一列表示一个质心，所以dists[i, j]就是第i个数据点到第j个质心的距离。
例如，dists[0, 0]表示第0个数据点到第0个质心的距离，计算方式就是计算两点的欧几里得距离，也就是sqrt((1.0-3.0)**2 + (2.0-1.0)**2) = 2.2361。
'''



'''
labels = dists.argmin(dim=1)

在PyTorch中，argmin()函数是用来返回在给定维度上最小值的索引的。当你使用argmin(dim=1)时，这就表示你希望在第1个维度（即，每一行内）找到最小值的索引。

对于上述例子中的dists张量：

tensor([[2.2361, 1.0000],
        [2.8284, 0.0000],
        [1.0000, 2.2361],
        [0.0000, 2.8284]])
当你调用dists.argmin(dim=1)时，你将得到：

tensor([1, 1, 0, 0])
这意味着：
对于第0个数据点（即，第0行），离它最近的质心是第1个质心（因为1.0000 < 2.2361）。
对于第1个数据点（即，第1行），离它最近的质心也是第1个质心（因为0.0000 < 2.8284）。
对于第2个数据点（即，第2行），离它最近的质心是第0个质心（因为1.0000 < 2.2361）。
对于第3个数据点（即，第3行），离它最近的质心也是第0个质心（因为0.0000 < 2.8284）。
换句话说，
这个labels张量就是我们根据每个数据点到每个质心的距离来分配每个数据点到最近的质心的结果。这个步骤是K-Means算法的"分配"步骤。
'''